{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from http://0.0.0.0:8888/edit/NYSK/lsa.jar\n",
      "Finished download of lsa.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Magic AddJar failed to execute with error: \n",
       "Jar 'lsa.jar' is not valid."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%AddJar http://0.0.0.0:8888/edit/NYSK/lsa.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of lsa.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Magic AddJar failed to execute with error: \n",
       "Jar 'lsa.jar' is not valid."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%AddJar http://cedric.cnam.fr/~ferecatu/RCP216/tp/tptexte/lsa.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:36: error: object cloudera is not a member of package com\n",
       "       import com.cloudera.datascience.common.XmlInputFormat\n",
       "                  ^\n",
       "<console>:37: error: object cloudera is not a member of package com\n",
       "       import com.cloudera.datascience.lsa.ParseWikipedia._\n",
       "                  ^\n",
       "<console>:38: error: object cloudera is not a member of package com\n",
       "       import com.cloudera.datascience.lsa.RunLSA._\n",
       "                  ^\n",
       "<console>:49: error: object stanford is not a member of package edu\n",
       "       import edu.stanford.nlp.ling.CoreLabel;\n",
       "                  ^\n",
       "<console>:50: error: object stanford is not a member of package edu\n",
       "       import edu.stanford.nlp.ling.HasWord;\n",
       "                  ^\n",
       "<console>:51: error: object stanford is not a member of package edu\n",
       "       import edu.stanford.nlp.process.CoreLabelTokenFactory;\n",
       "                  ^\n",
       "<console>:52: error: object stanford is not a member of package edu\n",
       "       import edu.stanford.nlp.process.DocumentPreprocessor;\n",
       "                  ^\n",
       "<console>:53: error: object stanford is not a member of package edu\n",
       "       import edu.stanford.nlp.process.PTBTokenizer;\n",
       "                  ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.rdd\n",
    "\n",
    "import scala.xml._\n",
    "\n",
    "import org.apache.hadoop.io.{ Text, LongWritable }\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import com.cloudera.datascience.common.XmlInputFormat\n",
    "import com.cloudera.datascience.lsa.ParseWikipedia._\n",
    "import com.cloudera.datascience.lsa.RunLSA._\n",
    "\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "import breeze.linalg.{DenseMatrix => BDenseMatrix, DenseVector => BDenseVector, SparseVector => BSparseVector, Vector => BVector}\n",
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "import java.io.StringReader\n",
    "import edu.stanford.nlp.ling.CoreLabel;\n",
    "import edu.stanford.nlp.ling.HasWord;\n",
    "import edu.stanford.nlp.process.CoreLabelTokenFactory;\n",
    "import edu.stanford.nlp.process.DocumentPreprocessor;\n",
    "import edu.stanford.nlp.process.PTBTokenizer;\n",
    "\n",
    "import java.sql.Timestamp\n",
    "import java.text.SimpleDateFormat\n",
    "\n",
    "import org.apache.spark.mllib.clustering.{ KMeans, KMeansModel }\n",
    "import org.apache.spark.mllib.util.KMeansDataGenerator\n",
    "\n",
    "import scala.collection.mutable.HashMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:27: error: not found: value BVector\n",
       "def toBreeze(v:Vector) = BVector(v.toArray)\n",
       "                         ^\n",
       "<console>:27: error: type Vector takes type parameters\n",
       "def toBreeze(v:Vector) = BVector(v.toArray)\n",
       "               ^\n",
       "<console>:28: error: not found: value Vectors\n",
       "def fromBreeze(bv:BVector[Double]) = Vectors.dense(bv.toArray)\n",
       "                                     ^\n",
       "<console>:28: error: not found: type BVector\n",
       "def fromBreeze(bv:BVector[Double]) = Vectors.dense(bv.toArray)\n",
       "                  ^\n",
       "<console>:29: error: type Vector takes type parameters\n",
       "def add(v1:Vector, v2:Vector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
       "           ^\n",
       "<console>:29: error: type Vector takes type parameters\n",
       "def add(v1:Vector, v2:Vector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
       "                      ^\n",
       "<console>:30: error: type Vector takes type parameters\n",
       "def scalarMultiply(a:Double, v:Vector) = fromBreeze(a * toBreeze(v))\n",
       "                               ^\n",
       "<console>:33: error: not found: value Vectors\n",
       "var v3 = Vectors.zeros(v1.size+v2.size)\n",
       "         ^\n",
       "<console>:32: error: type Vector takes type parameters\n",
       "def stackVectors(v1:Vector, v2:Vector) = {\n",
       "                    ^\n",
       "<console>:35: error: not found: value BVector\n",
       "      BVector(v3.toArray)(i) = v1(i);\n",
       "      ^\n",
       "<console>:32: error: type Vector takes type parameters\n",
       "def stackVectors(v1:Vector, v2:Vector) = {\n",
       "                               ^\n",
       "<console>:38: error: not found: value BVector\n",
       "      BVector(v3.toArray)(v1.size+i) = v2(i);\n",
       "      ^\n",
       "<console>:44: error: not found: type RDD\n",
       "def loadArticle(sc: SparkContext, path: String): RDD[String] = {\n",
       "                                                 ^\n",
       "<console>:44: error: not found: type SparkContext\n",
       "def loadArticle(sc: SparkContext, path: String): RDD[String] = {\n",
       "                    ^\n",
       "<console>:45: error: not found: type Configuration\n",
       "@transient val conf = new Configuration()\n",
       "                          ^\n",
       "<console>:61: error: not found: type SimpleDateFormat\n",
       "    val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n",
       "                     ^\n",
       "<console>:66: error: not found: type Timestamp\n",
       "      val t = new Timestamp(d.getTime());\n",
       "                  ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toBreeze(v:Vector) = BVector(v.toArray)\n",
    "def fromBreeze(bv:BVector[Double]) = Vectors.dense(bv.toArray)\n",
    "def add(v1:Vector, v2:Vector) = fromBreeze(toBreeze(v1) + toBreeze(v2))\n",
    "def scalarMultiply(a:Double, v:Vector) = fromBreeze(a * toBreeze(v))\n",
    "\n",
    "def stackVectors(v1:Vector, v2:Vector) = {\n",
    "var v3 = Vectors.zeros(v1.size+v2.size)\n",
    "    for (i <- 0 until v1.size) {\n",
    "      BVector(v3.toArray)(i) = v1(i);\n",
    "    }\n",
    "    for (i <- 0 until v2.size) {\n",
    "      BVector(v3.toArray)(v1.size+i) = v2(i);\n",
    "    }\n",
    "}\n",
    "\n",
    "// SÃ©paration du fichier XML en un RDD oÃ¹ chaque Ã©lÃ©ment est un article\n",
    "// Retourne un RDD de String Ã  partir du fichier \"path\"\n",
    "def loadArticle(sc: SparkContext, path: String): RDD[String] = {\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XmlInputFormat.START_TAG_KEY, \"<document>\")\n",
    "conf.set(XmlInputFormat.END_TAG_KEY, \"</document>\")\n",
    "val in = sc.newAPIHadoopFile(path, classOf[XmlInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "in.map(line => line._2.toString)\n",
    "}\n",
    "\n",
    "\n",
    "// Pour un élément XML de type \"document\",\n",
    "//   - on extrait le champ \"date\"\n",
    "//   - on parse la chaÃ®ne de caractÃ¨re au format yyyy-MM-dd HH:mm:ss\n",
    "//   - on retourne un Timestamp\n",
    "def extractDate(elem: scala.xml.Elem): java.sql.Timestamp = {\n",
    "    val dn: scala.xml.NodeSeq = elem \\\\ \"date\"\n",
    "    val x: String = dn.text\n",
    "    // d'aprÃ¨s l'exemple 2011-05-18 16:30:35\n",
    "    val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n",
    "    if (x == \"\")\n",
    "      return null\n",
    "    else {\n",
    "      val d = format.parse(x.toString());\n",
    "      val t = new Timestamp(d.getTime());\n",
    "      return t\n",
    "    }\n",
    "}\n",
    "\n",
    "// Pour un élément XML de type \"document\",\n",
    "//   - on extrait le champ #field\n",
    "def extractString(elem: scala.xml.Elem, field: String): String = {\n",
    "    val dn: scala.xml.NodeSeq = elem \\\\ field\n",
    "    val x: String = dn.text\n",
    "    return x\n",
    "}\n",
    "\n",
    "def extractInt(elem: scala.xml.Elem, field: String): Int = {\n",
    "    val dn: scala.xml.NodeSeq = elem \\\\ field\n",
    "    val x: Int = dn.text.toInt\n",
    "    return x\n",
    "}\n",
    "\n",
    "def extractAll(elem: scala.xml.Elem, whatText: String = \"text\"): (Int, java.sql.Timestamp, String) = {\n",
    "    return (extractInt(elem,\"docid\"), extractDate(elem), extractString(elem,whatText))\n",
    "}\n",
    "\n",
    "def extractText(elem: scala.xml.Elem): String = {\n",
    "    return (extractString(elem,\"title\") + \" \" + extractString(elem,\"summary\") + \" \" + extractString(elem,\"text\"))\n",
    "}\n",
    "\n",
    "// NÃ©cessaire, car le type java.sql.Timestamp n'est pas ordonnÃ© par dÃ©faut (étonnant...)\n",
    "implicit def ordered: Ordering[java.sql.Timestamp] = new Ordering[java.sql.Timestamp] {\n",
    "def compare(x: java.sql.Timestamp, y: java.sql.Timestamp): Int = x compareTo y\n",
    "}\n",
    "\n",
    "def hasLetters(str: String): Boolean = {\n",
    "// While loop for high performance\n",
    "var i = 0\n",
    "while (i < str.length) {\n",
    "  if (Character.isLetter(str.charAt(i))) {\n",
    "    return true\n",
    "  }\n",
    "  i += 1\n",
    "}\n",
    "false\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords = Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val stopwords = sc.textFile(\"stopwords.txt\").collect.toArray.toSet\n",
    "    val stopwordsBroadcast = sc.broadcast(stopwords).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.lang.AssertionError: assertion failed: \n",
      "  Object\n",
      "        with Serializable {\n",
      "  def $init$(): Unit\n",
      "  def size(): Int\n",
      "  def toArray(): Array[Double]\n",
      "  override def equals(other: Object): Boolean\n",
      "  override def hashCode(): Int\n",
      "  private[package spark] def asBreeze(): breeze.linalg.Vector\n",
      "  def apply(i: Int): Double\n",
      "  def copy(): org.apache.spark.mllib.linalg.Vector\n",
      "  def foreachActive(f: Function2): Unit\n",
      "  def numActives(): Int\n",
      "  def numNonzeros(): Int\n",
      "  def toSparse(): org.apache.spark.mllib.linalg.SparseVector\n",
      "  private[package linalg] def toSparseWithSize(nnz: Int): org.apache.spark.mllib.linalg.SparseVector\n",
      "  def toDense(): org.apache.spark.mllib.linalg.DenseVector\n",
      "  def compressed(): org.apache.spark.mllib.linalg.Vector\n",
      "  def argmax(): Int\n",
      "  def toJson(): String\n",
      "  def asML(): org.apache.spark.ml.linalg.Vector\n",
      "}\n",
      "     while compiling: <console>\n",
      "        during phase: globalPhase=terminal, enteringPhase=jvm\n",
      "     library version: version 2.11.12\n",
      "    compiler version: version 2.11.12\n",
      "  reconstructed args: -classpath file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/conf/:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-lang-2.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/libthrift-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stax-api-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jdo-api-3.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stax-api-1.0-2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-core-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xz-1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-httpclient-3.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-client-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-model-common-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xmlenc-0.52.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jpam-1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-cli-1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zookeeper-3.4.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/netty-all-4.1.17.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.1-tests.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/derby-10.12.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr-runtime-3.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.inject-1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/super-csv-2.2.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jsr305-1.3.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-format-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-lang3-3.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/httpclient-4.5.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/joda-time-2.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-column-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-shims-1.5.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-collections-3.2.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/okhttp-3.8.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/automaton-1.11-8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-server-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-repl_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javolution-5.5.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-io-2.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/gson-2.2.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/objenesis-2.5.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-logging-1.1.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-json-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guice-servlet-3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stream-2.7.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jetty-util-6.1.26.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/chill-java-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-format-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jetty-6.1.26.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/libfb303-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jsp-api-2.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr-2.7.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-core-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aopalliance-1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/core-1.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-sql_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jtransforms-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-1.8.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/minlog-1.3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-net-3.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/generex-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/lz4-java-1.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/httpcore-4.4.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/okio-1.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-client-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/pyrolite-4.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/janino-3.0.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-model-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/log4j-1.2.17.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/netty-3.9.9.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/opencsv-2.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-common-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-mapreduce-1.5.5-nohive.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guice-3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jta-1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-pool-1.5.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guava-14.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-hive_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-codec-1.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/ivy-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-digester-1.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-math3-3.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/py4j-0.10.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/oro-2.0.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-framework-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-core-1.5.5-nohive.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-common-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aircompressor-0.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jline-2.14.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jodd-core-3.5.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-core_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-compress-1.8.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-dbcp-1.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/activation-1.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/paranamer-2.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-databind-2.6.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hppc-0.7.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snakeyaml-1.15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/ST4-4.0.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snappy-0.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-client-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar:file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar -Yrepl-outdir /tmp/spark-16b89def-fb77-4c4a-ae38-f3f664151ea7/repl-65a943ab-4f3d-436c-965d-50e3acc17657 -Yrepl-class-based\n",
      "\n",
      "  last tree to typer: TypeTree(class Byte)\n",
      "       tree position: line 6 of <console>\n",
      "            tree tpe: Byte\n",
      "              symbol: (final abstract) class Byte in package scala\n",
      "   symbol definition: final abstract class Byte extends  (a ClassSymbol)\n",
      "      symbol package: scala\n",
      "       symbol owners: class Byte\n",
      "           call site: object $eval in package $line222 in package $line222\n",
      "\n",
      "<Cannot read source file>\n",
      "\tat scala.tools.nsc.transform.AddInterfaces$LazyImplClassType.implType$1(AddInterfaces.scala:196)\n",
      "\tat scala.tools.nsc.transform.AddInterfaces$LazyImplClassType.complete(AddInterfaces.scala:203)\n",
      "\tat scala.tools.nsc.transform.AddInterfaces$LazyImplClassType.load(AddInterfaces.scala:206)\n",
      "\tat scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.initRoot(SymbolLoaders.scala:239)\n",
      "\tat scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:224)\n",
      "\tat scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:227)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:1090)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter.recreateOrRelink$1(Importers.scala:163)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter.importSymbol(Importers.scala:210)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter$$anonfun$recreateType$4.apply(Importers.scala:248)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter$$anonfun$recreateType$4.apply(Importers.scala:248)\n",
      "\tat scala.reflect.internal.Scopes$Scope.foreach(Scopes.scala:373)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter.recreateType(Importers.scala:248)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter.importType(Importers.scala:284)\n",
      "\tat scala.reflect.internal.Importers$StandardImporter$$anon$1.complete(Importers.scala:75)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:174)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n",
      "\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:123)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:174)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$1.info(SynchronizedSymbols.scala:174)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.scala$reflect$internal$pickling$UnPickler$Scan$$fromName$1(UnPickler.scala:217)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readExtSymbol$1(UnPickler.scala:258)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readSymbol(UnPickler.scala:286)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readSymbolRef(UnPickler.scala:651)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readType(UnPickler.scala:419)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply(UnPickler.scala:660)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan$$anonfun$readTypeRef$1.apply(UnPickler.scala:660)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.at(UnPickler.scala:179)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readTypeRef(UnPickler.scala:660)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readType(UnPickler.scala:422)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply(UnPickler.scala:734)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef$$anonfun$7.apply(UnPickler.scala:734)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.at(UnPickler.scala:179)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.completeInternal(UnPickler.scala:734)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:761)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:186)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n",
      "\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:123)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:186)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$9.info(SynchronizedSymbols.scala:186)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1680)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.annotations(Symbols.scala:1847)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.addAnnotation(Symbols.scala:1866)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.readSymbolAnnotation(UnPickler.scala:517)\n",
      "\tat scala.reflect.internal.pickling.UnPickler$Scan.run(UnPickler.scala:97)\n",
      "\tat scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:38)\n",
      "\tat scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:619)\n",
      "\tat scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp(SymbolLoaders.scala:28)\n",
      "\tat scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply(SymbolLoaders.scala:25)\n",
      "\tat scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply(SymbolLoaders.scala:25)\n",
      "\tat scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:263)\n",
      "\tat scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:25)\n",
      "\tat scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:189)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n",
      "\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:123)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:189)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info(SynchronizedSymbols.scala:127)\n",
      "\tat scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info(SynchronizedSymbols.scala:189)\n",
      "\tat scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)\n",
      "\tat scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)\n",
      "\tat scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:66)\n",
      "\tat scala.reflect.internal.Mirrors$RootsBase.staticModuleOrClass(Mirrors.scala:77)\n",
      "\tat scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:122)\n",
      "\tat scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:22)\n",
      "\tat org.apache.spark.mllib.feature.Word2VecModel$SaveLoadV1_0$$typecreator1$1.apply(Word2Vec.scala:664)\n",
      "\tat scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:232)\n",
      "\tat scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:232)\n",
      "\tat org.apache.spark.sql.catalyst.ScalaReflection$class.localTypeOf(ScalaReflection.scala:921)\n",
      "\tat org.apache.spark.sql.catalyst.ScalaReflection$.localTypeOf(ScalaReflection.scala:46)\n",
      "\tat org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:720)\n",
      "\tat org.apache.spark.mllib.util.Loader$.checkSchema(modelSaveLoad.scala:110)\n",
      "\tat org.apache.spark.mllib.feature.Word2VecModel$SaveLoadV1_0$.load(Word2Vec.scala:664)\n",
      "\tat org.apache.spark.mllib.feature.Word2VecModel$.load(Word2Vec.scala:706)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:157)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:163)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:165)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:167)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:169)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:171)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:173)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:175)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:177)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:179)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:181)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:183)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:185)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:187)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:189)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:191)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:193)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:195)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:197)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:199)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:201)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:203)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:205)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:207)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:209)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:211)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:213)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:215)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:217)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:219)\n",
      "\tat $line222.$read$$iw$$iw$$iw$$iw.<init>(<console>:221)\n",
      "\tat $line222.$read$$iw$$iw$$iw.<init>(<console>:223)\n",
      "\tat $line222.$read$$iw$$iw.<init>(<console>:225)\n",
      "\tat $line222.$read$$iw.<init>(<console>:227)\n",
      "\tat $line222.$read.<init>(<console>:229)\n",
      "\tat $line222.$read$.<init>(<console>:233)\n",
      "\tat $line222.$read$.<clinit>(<console>)\n",
      "\tat $line222.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line222.$eval$.$print(<console>:6)\n",
      "\tat $line222.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreterSpecific.scala:385)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreterSpecific.scala:380)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withErr(Console.scala:80)\n",
      "\tat org.apache.toree.global.StreamState$$anonfun$1$$anonfun$apply$1.apply(StreamState.scala:73)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withOut(Console.scala:53)\n",
      "\tat org.apache.toree.global.StreamState$$anonfun$1.apply(StreamState.scala:72)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withIn(Console.scala:124)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:71)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1.apply(ScalaInterpreterSpecific.scala:379)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1.apply(ScalaInterpreterSpecific.scala:379)\n",
      "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$2.run(TaskManager.scala:134)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: error reading Scala signature of org.apache.spark.mllib.feature.Word2VecModel: assertion failed: \n",
       "  Object\n",
       "        with Serializable {\n",
       "  def $init$(): Unit\n",
       "  def size(): Int\n",
       "  def toArray(): Array[Double]\n",
       "  override def equals(other: Object): Boolean\n",
       "  override def hashCode(): Int\n",
       "  private[package spark] def asBreeze(): breeze.linalg.Vector\n",
       "  def apply(i: Int): Double\n",
       "  def copy(): org.apache.spark.mllib.linalg.Vector\n",
       "  def foreachActive(f: Function2): Unit\n",
       "  def numActives(): Int\n",
       "  def numNonzeros(): Int\n",
       "  def toSparse(): org.apache.spark.mllib.linalg.SparseVector\n",
       "  private[package linalg] def toSparseWithSize(nnz: Int): org.apache.spark.mllib.linalg.SparseVector\n",
       "  def toDense(): org.apache.spark.mllib.linalg.DenseVector\n",
       "  def compressed(): org.apache.spark.mllib.linalg.Vector\n",
       "  def argmax(): Int\n",
       "  def toJson(): String\n",
       "  def asML(): org.apache.spark.ml.linalg.Vector\n",
       "}\n",
       "     while compiling: <console>\n",
       "        during phase: globalPhase=terminal, enteringPhase=jvm\n",
       "     library version: version 2.11.12\n",
       "    compiler version: version 2.11.12\n",
       "  reconstructed args: -classpath file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/conf/:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-lang-2.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/libthrift-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stax-api-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jdo-api-3.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stax-api-1.0-2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-core-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xz-1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-httpclient-3.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-client-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-model-common-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xmlenc-0.52.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jpam-1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-cli-1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zookeeper-3.4.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/netty-all-4.1.17.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.1-tests.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/derby-10.12.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr-runtime-3.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.inject-1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/super-csv-2.2.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jsr305-1.3.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-format-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-lang3-3.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/httpclient-4.5.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/joda-time-2.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-column-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-shims-1.5.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-collections-3.2.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/okhttp-3.8.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/automaton-1.11-8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-server-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-repl_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javolution-5.5.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-io-2.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/gson-2.2.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/objenesis-2.5.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-logging-1.1.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-json-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guice-servlet-3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stream-2.7.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jetty-util-6.1.26.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/chill-java-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-format-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jetty-6.1.26.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/libfb303-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jsp-api-2.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr-2.7.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-core-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aopalliance-1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/core-1.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-sql_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jtransforms-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-1.8.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/minlog-1.3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-net-3.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/generex-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/lz4-java-1.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/httpcore-4.4.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/okio-1.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-client-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/pyrolite-4.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/janino-3.0.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-model-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/log4j-1.2.17.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/netty-3.9.9.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/opencsv-2.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-common-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-mapreduce-1.5.5-nohive.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guice-3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jta-1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-pool-1.5.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guava-14.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-hive_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-codec-1.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/ivy-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-digester-1.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-math3-3.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/py4j-0.10.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/oro-2.0.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-framework-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-core-1.5.5-nohive.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-common-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aircompressor-0.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jline-2.14.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jodd-core-3.5.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-core_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-compress-1.8.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-dbcp-1.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/activation-1.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/paranamer-2.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-databind-2.6.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hppc-0.7.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snakeyaml-1.15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/ST4-4.0.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snappy-0.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-client-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar:file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar -Yrepl-outdir /tmp/spark-16b89def-fb77-4c4a-ae38-f3f664151ea7/repl-65a943ab-4f3d-436c-965d-50e3acc17657 -Yrepl-class-based\n",
       "\n",
       "  last tree to typer: TypeTree(class Byte)\n",
       "       tree position: line 6 of <console>\n",
       "            tree tpe: Byte\n",
       "              symbol: (final abstract) class Byte in package scala\n",
       "   symbol definition: final abstract class Byte extends  (a ClassSymbol)\n",
       "      symbol package: scala\n",
       "       symbol owners: class Byte\n",
       "           call site: object $eval in package $line222 in package $line222\n",
       "\n",
       "<Cannot read source file>\n",
       "StackTrace: This symbol is required by 'class org.apache.spark.mllib.feature.IDF'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'IDF.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n",
       "java.lang.RuntimeException: error reading Scala signature of org.apache.spark.mllib.feature.Word2VecModel: assertion failed:\n",
       "  Object\n",
       "        with Serializable {\n",
       "  def $init$(): Unit\n",
       "  def size(): Int\n",
       "  def toArray(): Array[Double]\n",
       "  override def equals(other: Object): Boolean\n",
       "  override def hashCode(): Int\n",
       "  private[package spark] def asBreeze(): breeze.linalg.Vector\n",
       "  def apply(i: Int): Double\n",
       "  def copy(): org.apache.spark.mllib.linalg.Vector\n",
       "  def foreachActive(f: Function2): Unit\n",
       "  def numActives(): Int\n",
       "  def numNonzeros(): Int\n",
       "  def toSparse(): org.apache.spark.mllib.linalg.SparseVector\n",
       "  private[package linalg] def toSparseWithSize(nnz: Int): org.apache.spark.mllib.linalg.SparseVector\n",
       "  def toDense(): org.apache.spark.mllib.linalg.DenseVector\n",
       "  def compressed(): org.apache.spark.mllib.linalg.Vector\n",
       "  def argmax(): Int\n",
       "  def toJson(): String\n",
       "  def asML(): org.apache.spark.ml.linalg.Vector\n",
       "}\n",
       "     while compiling: <console>\n",
       "        during phase: globalPhase=terminal, enteringPhase=jvm\n",
       "     library version: version 2.11.12\n",
       "    compiler version: version 2.11.12\n",
       "  reconstructed args: -classpath file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:file:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/conf/:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-lang-2.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/libthrift-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stax-api-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jdo-api-3.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stax-api-1.0-2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-core-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xz-1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-httpclient-3.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-client-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-model-common-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xmlenc-0.52.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jpam-1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-cli-1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zookeeper-3.4.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/netty-all-4.1.17.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.1-tests.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snappy-java-1.1.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/derby-10.12.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr-runtime-3.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.inject-1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/super-csv-2.2.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jsr305-1.3.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-format-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-lang3-3.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/httpclient-4.5.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/joda-time-2.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-column-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-shims-1.5.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-collections-3.2.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/okhttp-3.8.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/automaton-1.11-8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-server-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-repl_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javolution-5.5.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-io-2.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/gson-2.2.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/objenesis-2.5.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-logging-1.1.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-json-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guice-servlet-3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stream-2.7.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jetty-util-6.1.26.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/chill-java-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-format-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jetty-6.1.26.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/libfb303-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jsp-api-2.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-library-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr-2.7.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-core-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aopalliance-1.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/core-1.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-sql_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jtransforms-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-1.8.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/minlog-1.3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-net-3.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/generex-1.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/lz4-java-1.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/httpcore-4.4.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/okio-1.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-client-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/pyrolite-4.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/janino-3.0.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kubernetes-model-4.1.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-tags_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/log4j-1.2.17.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/netty-3.9.9.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/opencsv-2.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-common-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-mapreduce-1.5.5-nohive.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guice-3.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jta-1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-pool-1.5.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/guava-14.0.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-hive_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-codec-1.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/ivy-2.4.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-digester-1.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-math3-3.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/py4j-0.10.7.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/oro-2.0.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-framework-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/orc-core-1.5.5-nohive.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-common-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/aircompressor-0.10.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jline-2.14.6.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jodd-core-3.5.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-core_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-compress-1.8.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/commons-dbcp-1.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/activation-1.1.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/paranamer-2.8.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/jackson-databind-2.6.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hppc-0.7.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snakeyaml-1.15.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/ST4-4.0.4.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/snappy-0.2.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/curator-client-2.7.1.jar:file:/usr/local/spark-2.4.1-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar:file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar -Yrepl-outdir /tmp/spark-16b89def-fb77-4c4a-ae38-f3f664151ea7/repl-65a943ab-4f3d-436c-965d-50e3acc17657 -Yrepl-class-based\n",
       "  last tree to typer: TypeTree(class Byte)\n",
       "       tree position: line 6 of <console>\n",
       "            tree tpe: Byte\n",
       "              symbol: (final abstract) class Byte in package scala\n",
       "   symbol definition: final abstract class Byte extends  (a ClassSymbol)\n",
       "      symbol package: scala\n",
       "       symbol owners: class Byte\n",
       "           call site: object $eval in package $line222 in package $line222\n",
       "<Cannot read source file>\n",
       "  at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:46)\n",
       "  at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:619)\n",
       "  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply$mcV$sp(SymbolLoaders.scala:28)\n",
       "  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply(SymbolLoaders.scala:25)\n",
       "  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter$$anonfun$complete$1.apply(SymbolLoaders.scala:25)\n",
       "  at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:263)\n",
       "  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:25)\n",
       "  at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1535)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:189)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)\n",
       "  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n",
       "  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:123)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:189)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info(SynchronizedSymbols.scala:127)\n",
       "  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$10.info(SynchronizedSymbols.scala:189)\n",
       "  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)\n",
       "  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)\n",
       "  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:66)\n",
       "  at scala.reflect.internal.Mirrors$RootsBase.staticModuleOrClass(Mirrors.scala:77)\n",
       "  at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:122)\n",
       "  at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:22)\n",
       "  at org.apache.spark.mllib.feature.Word2VecModel$SaveLoadV1_0$$typecreator1$1.apply(Word2Vec.scala:664)\n",
       "  at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:232)\n",
       "  at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:232)\n",
       "  at org.apache.spark.sql.catalyst.ScalaReflection$class.localTypeOf(ScalaReflection.scala:921)\n",
       "  at org.apache.spark.sql.catalyst.ScalaReflection$.localTypeOf(ScalaReflection.scala:46)\n",
       "  at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:720)\n",
       "  at org.apache.spark.mllib.util.Loader$.checkSchema(modelSaveLoad.scala:110)\n",
       "  at org.apache.spark.mllib.feature.Word2VecModel$SaveLoadV1_0$.load(Word2Vec.scala:664)\n",
       "  at org.apache.spark.mllib.feature.Word2VecModel$.load(Word2Vec.scala:706)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.Word2VecModel\n",
    "// lire le Word2VecModel\n",
    "val w2vModel = Word2VecModel.load(sc, \"w2vModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\n",
       "org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
       "org.apache.toree.kernel.api.Kernel.sparkSession(Kernel.scala:444)\n",
       "org.apache.toree.kernel.api.Kernel.sparkContext(Kernel.scala:449)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
       "org.apache.toree.kernel.api.Kernel.addJars(Kernel.scala:80)\n",
       "org.apache.toree.magic.builtin.AddJar.execute(AddJar.scala:181)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.toree.plugins.PluginMethod$$anonfun$invoke$2.apply(PluginMethod.scala:116)\n",
       "scala.util.Try$.apply(Try.scala:192)\n",
       "org.apache.toree.plugins.PluginMethod.invoke(PluginMethod.scala:84)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:334)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:333)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "StackTrace: org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
       "org.apache.toree.kernel.api.Kernel.sparkSession(Kernel.scala:444)\n",
       "org.apache.toree.kernel.api.Kernel.sparkContext(Kernel.scala:449)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "org.apache.toree.kernel.api.Kernel$$anonfun$addJars$3.apply(Kernel.scala:80)\n",
       "scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
       "org.apache.toree.kernel.api.Kernel.addJars(Kernel.scala:80)\n",
       "org.apache.toree.magic.builtin.AddJar.execute(AddJar.scala:181)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.toree.plugins.PluginMethod$$anonfun$invoke$2.apply(PluginMethod.scala:116)\n",
       "scala.util.Try$.apply(Try.scala:192)\n",
       "org.apache.toree.plugins.PluginMethod.invoke(PluginMethod.scala:84)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:334)\n",
       "org.apache.toree.plugins.PluginManager$$anonfun$8.apply(PluginManager.scala:333)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2483)\n",
       "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2479)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2479)\n",
       "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2568)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"NYSK\").setMaster(\"local[*]\").set(\"spark.driver.memory\", \"10g\")\n",
    "\n",
    "\n",
    "\n",
    "conf.set(\"spark.hadoop.validateOutputSpecs\", \"false\")\n",
    "//conf.set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "//conf.setMaster(\"local[*]\")\n",
    "//conf.set(\"spark.executor.memory\", MAX_MEMORY)\n",
    "//conf.set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "//conf.set(\"spark.driver.maxResultSize\", MAX_MEMORY)\n",
    "val sc = new SparkContext(conf)\n",
    "\n",
    "\n",
    "val hadoopConf = new org.apache.hadoop.conf.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textToExtract = text\n",
       "useW2Vec = true\n",
       "weightTfIdf = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    var textToExtract = \"text\";\n",
    "    var useW2Vec = true;\n",
    "    var weightTfIdf = true;\n",
    "    println (\"*****\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nysk_raw = MapPartitionsRDD[55] at map at <console>:180\n",
       "nysk_xml = MapPartitionsRDD[56] at map at <console>:159\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[56] at map at <console>:159"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val nysk_raw = loadArticle(sc, \"nysk.xml\")/*.sample(false,0.01)*/\n",
    "    val nysk_xml: RDD[Elem] = nysk_raw.map(XML.loadString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nysk = MapPartitionsRDD[57] at map at <console>:158\n",
       "nyskTitles = MapPartitionsRDD[58] at map at <console>:159\n",
       "nyskSummaries = MapPartitionsRDD[59] at map at <console>:160\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[59] at map at <console>:160"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val nysk: RDD[(Int, java.sql.Timestamp, String)] = nysk_xml.map(e => extractAll(e,textToExtract))\n",
    "    val nyskTitles: RDD[(Int, java.sql.Timestamp, String)] = nysk_xml.map(e => extractAll(e,\"title\"))\n",
    "    val nyskSummaries: RDD[(Int, java.sql.Timestamp, String)] = nysk_xml.map(e => extractAll(e,\"summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords = Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Set(down, it's, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, ours\tourselves, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, w..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val stopwords = sc.textFile(\"stopwords.txt\").collect.toArray.toSet\n",
    "    val stopwordsBroadcast = sc.broadcast(stopwords).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemmatizedWithDate = MapPartitionsRDD[62] at mapPartitions at <console>:157\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[62] at mapPartitions at <console>:157"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   val lemmatizedWithDate = nysk.mapPartitions(iter => {\n",
    "      val pipeline = com.cloudera.datascience.lsa.ParseWikipedia.createNLPPipeline();\n",
    "      iter.map {\n",
    "        case (docid, date, text) =>\n",
    "          (docid.toString, date,\n",
    "            com.cloudera.datascience.lsa.ParseWikipedia.plainTextToLemmas(text.toLowerCase.split(\"\\\\W+\").mkString(\" \"), stopwordsBroadcast, pipeline))\n",
    "        };\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemmatized = MapPartitionsRDD[63] at map at <console>:155\n",
       "numTerms = 1000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val lemmatized = lemmatizedWithDate.map { case (docid, date, text) => (docid, text) }\n",
    "    val numTerms = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Number of terms: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "termDocMatrix = MapPartitionsRDD[74] at map at ParseWikipedia.scala:64\n",
       "termIds = Map(645 -> race, 892 -> tell, 69 -> across, 809 -> fund, 629 -> chance, 365 -> nearly, 138 -> less, 760 -> file, 101 -> claim, 479 -> key, 347 -> within, 846 -> speculation, 909 -> major, 333 -> minister, 628 -> opinion, 249 -> bathroom, 893 -> word, 518 -> loan, 962 -> suite, 468 -> obama, 234 -> german, 941 -> singapore, 0 -> incident, 777 -> prison, 555 -> north, 666 -> management, 88 -> guinea, 481 -> now, 352 -> criminal, 408 -> strong, 977 -> month, 170 -> decide, 523 -> believe, 582 -> wall, 762 -> control, 115 -> step, 683 -> appearance, 730 -> nature, 217 -> world, 276 -> prepare, 994 -> decline, 308 -...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(645 -> race, 892 -> tell, 69 -> across, 809 -> fund, 629 -> chance, 365 -> nearly, 138 -> less, 760 -> file, 101 -> claim, 479 -> key, 347 -> within, 846 -> speculation, 909 -> major, 333 -> minister, 628 -> opinion, 249 -> bathroom, 893 -> word, 518 -> loan, 962 -> suite, 468 -> obama, 234 -> german, 941 -> singapore, 0 -> incident, 777 -> prison, 555 -> north, 666 -> management, 88 -> guinea, 481 -> now, 352 -> criminal, 408 -> strong, 977 -> month, 170 -> decide, 523 -> believe, 582 -> wall, 762 -> control, 115 -> step, 683 -> appearance, 730 -> nature, 217 -> world, 276 -> prepare, 994 -> decline, 308 -..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    val (termDocMatrix, termIds, docIds, idfs) = com.cloudera.datascience.lsa.ParseWikipedia.termDocumentMatrix(lemmatized, stopwordsBroadcast, numTerms, sc);\n",
    "    termDocMatrix.cache();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:159: error: not enough arguments for method collect: (pf: PartialFunction[String,B])(implicit bf: scala.collection.generic.CanBuildFrom[Iterable[String],B,That])That.\n",
       "Unspecified value parameter pf.\n",
       "       termIdsTxT.collect()\n",
       "                         ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termIdsTxT = termIds.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "val outputtermIds= \"data/termIds.txt\"\n",
    "termIdsTxT.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:25: error: not found: type RowMatrix\n",
       "       val mat = new RowMatrix(termDocMatrix)\n",
       "                     ^\n",
       "<console>:25: error: not found: value termDocMatrix\n",
       "       val mat = new RowMatrix(termDocMatrix)\n",
       "                               ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mat = new RowMatrix(termDocMatrix)\n",
    "val k = 10 // nombre de valeurs singuliÃ¨res Ã  garder\n",
    "val svd = mat.computeSVD(k, computeU=true)\n",
    "val projections = mat.multiply(svd.V)\n",
    "val projectionsTxt = projections.rows.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "// Delete the existing path, ignore any exceptions thrown if the path doesn't exist\n",
    "val outputProjection = \"data/projection_LSA.txt\"\n",
    "\n",
    "projectionsTxt.saveAsTextFile(outputProjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbClusters = 10\n",
       "nbIterations = 1000\n",
       "runs = 10\n",
       "clustering = org.apache.spark.mllib.clustering.KMeansModel@669cb1c2\n",
       "classes = MapPartitionsRDD[146] at map at KMeansModel.scala:85\n",
       "outputClasses = data/classes_LSA.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data/classes_LSA.txt"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        val nbClusters = 10\n",
    "        val nbIterations = 1000\n",
    "        val runs = 10\n",
    "        val clustering = KMeans.train(termDocMatrix, nbClusters, nbIterations, runs, \"k-means||\", 0)\n",
    "        /*val outputClustering = \"hdfs://head.local:9000/user/emeric/clusters\"\n",
    "        try { hdfs.delete(new org.apache.hadoop.fs.Path(outputClustering), true) } \n",
    "        catch { case _ : Throwable => { } }\n",
    "        clustering.save(sc, outputClustering)*/\n",
    "        \n",
    "        val classes = clustering.predict(termDocMatrix)\n",
    "        val outputClasses = \"data/classes_LSA.txt\"\n",
    "    \n",
    "        classes.saveAsTextFile(outputClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n",
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "outputData = MapPartitionsRDD[161] at map at <console>:196\n",
       "outputDataFile = output/output_LSA.txt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "output/output_LSA.txt"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "        val outputData = lemmatizedWithDate.zip(classes).map { case ((docid, date, title),cl) => (docid, date, cl) }.sortBy(_._2).map(l => l.toString.filter(c => c != '(' & c != ')'))\n",
    "        val outputDataFile = \"data/output_LSA.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputDataFile), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        outputData.saveAsTextFile(outputDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "comment,news,woman,obama,man,dsk,sarkozy,know,article,france,\n",
      "*****\n",
      "lagarde,european,minister,finance,candidate,europe,bank,economy,imf,emerge,\n",
      "*****\n",
      "percent,stock,index,market,price,fall,higher,euro,investor,rise,\n",
      "*****\n",
      "opinion,obama,geithner,scandal,politics,limit,tax,another,previous,rich,\n",
      "*****\n",
      "reuter,news,site,copyright,comment,mobile,com,index,capital,review,\n",
      "*****\n",
      "bail,court,lawyer,manhattan,apartment,judge,release,prosecutor,jail,attorney,\n",
      "*****\n",
      "building,sunday,sinclair,apartment,million,stand,manhattan,bond,outside,suite,\n",
      "*****\n",
      "request,information,find,incident,serious,rate,michael,eye,widely,secretary,\n",
      "*****\n",
      "com,search,email,update,alert,quote,address,rss,feed,london,\n",
      "*****\n",
      "please,person,don,anyone,language,clean,another,tweet,conduct,avoid,\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "        clustering.clusterCenters.foreach(clusterCenter => {\n",
    "            val highest = clusterCenter.toArray.zipWithIndex.sortBy(-_._1).map(v => v._2).take(10)\n",
    "            println(\"*****\")\n",
    "            highest.foreach { s => print( termIds(s) + \",\" ) }\n",
    "            println ()\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w2vModel = org.apache.spark.mllib.feature.Word2VecModel@78c028b5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.catalyst.QualifiedTableName'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'QualifiedTableName.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.Word2VecModel@78c028b5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.Word2VecModel\n",
    "// lire le Word2VecModel\n",
    "val w2vModel = Word2VecModel.load(sc, \"w2vModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vectors = w2vModel.getVectors.mapValues(vv => org.apache.spark.mllib.linalg.Vectors.dense(vv.map(_.toDouble))).map(identity)\n",
    "        // transmettre la map aux noeuds de calcul\n",
    "val bVectors = sc.broadcast(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:216: error: not found: value bVectors2\n",
       "                          val wvec = bVectors2.value.get(word)\n",
       "                                     ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "        val idTerms = termIds.map(_.swap)\n",
    "\n",
    "        val pairs = termDocMatrix.zip(lemmatized);\n",
    "\n",
    "        var w2vecRepr = pairs.map({ case (row, (docid, lemmas)) => \n",
    "          var vSum = Vectors.zeros(100)\n",
    "          var totalWeight: Double = 0\n",
    "          val words = lemmas.toSet\n",
    "          words.foreach { word =>\n",
    "               val colId = idTerms.get(word);\n",
    "               if (colId != None) {\n",
    "                   var weight = row(colId.get);\n",
    "                   if (! weightTfIdf) {\n",
    "                       weight = 1\n",
    "                   }\n",
    "                   val wvec = bVectors2.value.get(word)\n",
    "                   if (wvec != None) {          \n",
    "                       vSum = add(scalarMultiply(weight, wvec.get), vSum)\n",
    "                       totalWeight += weight\n",
    "                   }\n",
    "               }\n",
    "          }\n",
    "          if (totalWeight > 0) {\n",
    "               vSum = scalarMultiply(1.0 / totalWeight, vSum)\n",
    "          }\n",
    "          else {\n",
    "               vSum = Vectors.zeros(100);\n",
    "          }\n",
    "          (docid, vSum)\n",
    "        })/*.filter(vec => Vectors.norm(vec._2, 1.0) > 0.0)*/.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:194: error: not found: value w2vecRepr\n",
       "       val matRDD = w2vecRepr.map{v => v._2}.cache()\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matRDD = w2vecRepr.map{v => v._2}.cache()\n",
    "        val mat = new RowMatrix(matRDD)\n",
    "        val matrixTxt = mat.rows.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "        // Delete the existing path, ignore any exceptions thrown if the path doesn't exist\n",
    "        val outputMatrix = \"data/matrice_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputMatrix), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        matrixTxt.saveAsTextFile(outputMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        val centRed = new StandardScaler(withMean = true, withStd = true).fit(matRDD)\n",
    "        val matCR: RowMatrix = new RowMatrix(centRed.transform(matRDD))\n",
    "\n",
    "        val matCompPrinc = matCR.computePrincipalComponents(10)\n",
    "        val projections = matCR.multiply(matCompPrinc)\n",
    "        //val matSummary = projections.computeColumnSummaryStatistics()\n",
    "        val projectionsTxt = projections.rows.map(l => l.toString.filter(c => c != '[' & c != ']'))\n",
    "        // Delete the existing path, ignore any exceptions thrown if the path doesn't exist\n",
    "        val outputProjection = \"data/projection_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputProjection), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        projectionsTxt.saveAsTextFile(outputProjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     val nbClusters = 10\n",
    "        val nbIterations = 1000\n",
    "        val runs = 10\n",
    "        val clustering = KMeans.train(matRDD, nbClusters, nbIterations, runs, \"k-means||\", 0)\n",
    "        /*val outputClustering = \"hdfs://head.local:9000/user/emeric/clusters\"\n",
    "        try { hdfs.delete(new org.apache.hadoop.fs.Path(outputClustering), true) } \n",
    "        catch { case _ : Throwable => { } }\n",
    "        clustering.save(sc, outputClustering)*/\n",
    "        \n",
    "        val classes = clustering.predict(matRDD)\n",
    "        val outputClasses = \"data/classes_W2V.txt\"\n",
    "        //try { hdfs.delete(new org.apache.hadoop.fs.Path(outputClasses), true) } \n",
    "        //catch { case _ : Throwable => { } }\n",
    "        classes.saveAsTextFile(outputClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
